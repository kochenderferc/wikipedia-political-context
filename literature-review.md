### Penney, J. W. (2016). Chilling Effects: Online Surveillance and Wikipedia Use. Btlj. https://doi.org/10.15779/Z38SS13
This study showed that awareness of surveillance of IP address impacted Wikipedia usage, which is relevant because it means that potential editors/admins in more repressive countries will not participate to the full extent due to worries of surveillance. The paper uses empirical backing to show the cost of spam control, with limits on edits, potentially especially for politically controversial pages.
### Wikipedia world view “shaped by editors in the West”: University of Oxford. (n.d.). Retrieved from https://www.ox.ac.uk/news/2015-09-15-wikipedia-world-view-shaped-editors-west
This study from Oxford found that the majority of articles about places were edited by people living in the UK, USA, France, Germany, and Italy. Digital connectivity is a contributing factor, along with high income levels.
### Parra, Guillermo. (2024). Righting the Writers: Assessing Bias in Wikipedia's Political Content -An Event Study and Sentiment Analysis. http://dx.doi.org/10.2139/ssrn.5022797
This study uses a casual inference framework and showed that politicians who switched from left to right wing parties had a drop in sentiment on their Wikipedia articles, which could show systematic political bias. This means that even with Wikipedia's monitoring of vandalism there is bias, making it important for us to study the power of admins. One of the interesting parts of this paper was that they used the Wayback Machine API in addition to Wikipedia.
### de Laat, P.B. From open-source software to Wikipedia: ‘Backgrounding’ trust by collective monitoring and reputation tracking. Ethics Inf Technol 16, 157–169 (2014). https://doi.org/10.1007/s10676-014-9342-9
This paper goes into how Wikipedia monitors edits in order to avoid vandalism, with both admins and autonomous bots. It states that Wikipedia experiences roughly 9000 vandalism edits daily (7% of all edits). It elaborates on institutional trust, which in this context is how Wikipedia approaches trusting contributors. Wikipedia uses backgrounding trust, which requires monitoring reputation and uses corrective mechanisms in the background to enforce norms, with sanctions placed on users who deviate. Bots are used to highlight edits that are potentially vandalism, which raises the question of bias within the automated bots.
